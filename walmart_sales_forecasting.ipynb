{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment: Walmart Sales Forecasting\n",
        "\n",
        "This notebook builds a full demand-forecasting pipeline on `walmart_sales.csv` with:\n",
        "- Statistical demand equation estimation and parametric tests\n",
        "- Tree-based models: Random Forest, Extra Trees, XGBoost\n",
        "- Ensemble model: Voting Regressor\n",
        "- Visual diagnostics, heatmaps, and forecast quality plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['MPLCONFIGDIR'] = '/tmp/mpl'\n",
        "os.makedirs('/tmp/mpl', exist_ok=True)\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, VotingRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.base import clone\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "from statsmodels.stats.stattools import durbin_watson, jarque_bera\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.tools.sm_exceptions import InterpolationWarning\n",
        "\n",
        "warnings.filterwarnings('ignore', category=InterpolationWarning)\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBRegressor\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_context('notebook')\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "DATA_PATH = Path('/Users/panshulaj/Documents/sales-forecasting-walmart/data/walmart_sales.csv')\n",
        "OUT_DIR = Path('/Users/panshulaj/Documents/sale forecasting/outputs')\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('HAS_XGB =', HAS_XGB)\n",
        "print('Data path exists =', DATA_PATH.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and inspect data\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values(['Store', 'Date']).reset_index(drop=True)\n",
        "\n",
        "print('Shape:', df.shape)\n",
        "print('Columns:', list(df.columns))\n",
        "print()\n",
        "print('Missing values by column:')\n",
        "print(df.isna().sum())\n",
        "print()\n",
        "print('Date range:', df['Date'].min(), 'to', df['Date'].max())\n",
        "\n",
        "summary = df.describe(include='all').T\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Analysis\n",
        "We visualize the overall series behavior, feature correlations, and seasonality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate trend\n",
        "weekly_total = df.groupby('Date', as_index=False)['Weekly_Sales'].sum()\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "axes[0, 0].plot(weekly_total['Date'], weekly_total['Weekly_Sales'], color='#1f77b4')\n",
        "axes[0, 0].set_title('Total Weekly Sales Over Time')\n",
        "axes[0, 0].set_xlabel('Date')\n",
        "axes[0, 0].set_ylabel('Weekly Sales')\n",
        "\n",
        "sns.histplot(df['Weekly_Sales'], bins=50, kde=True, ax=axes[0, 1], color='#2ca02c')\n",
        "axes[0, 1].set_title('Distribution of Weekly Sales')\n",
        "\n",
        "corr_cols = ['Weekly_Sales', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "corr = df[corr_cols].corr()\n",
        "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Correlation Heatmap')\n",
        "\n",
        "monthly = df.assign(Month=df['Date'].dt.month).groupby('Month', as_index=False)['Weekly_Sales'].mean()\n",
        "sns.barplot(data=monthly, x='Month', y='Weekly_Sales', ax=axes[1, 1], palette='viridis')\n",
        "axes[1, 1].set_title('Average Sales by Month')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_DIR / 'eda_overview.png', dpi=160)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stationarity Checks (Non-Stationarity vs Log-Stationarity)\n",
        "We test stationarity using both:\n",
        "- ADF (null: non-stationary / unit root)\n",
        "- KPSS (null: stationary)\n",
        "\n",
        "This section checks raw sales, log-sales, and differenced variants.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate stationarity checks\n",
        "\n",
        "agg = df.groupby('Date', as_index=False)['Weekly_Sales'].sum().sort_values('Date')\n",
        "agg['log_sales'] = np.log(agg['Weekly_Sales'])\n",
        "agg['diff_sales'] = agg['Weekly_Sales'].diff()\n",
        "agg['diff_log_sales'] = agg['log_sales'].diff()\n",
        "\n",
        "def stationarity_test(series, name):\n",
        "    s = pd.Series(series).dropna()\n",
        "    out = {'Series': name}\n",
        "    adf_stat, adf_p, *_ = adfuller(s, autolag='AIC')\n",
        "    out['ADF_stat'] = adf_stat\n",
        "    out['ADF_p'] = adf_p\n",
        "    out['ADF_stationary_at_5pct'] = adf_p < 0.05\n",
        "    try:\n",
        "        kpss_stat, kpss_p, *_ = kpss(s, regression='c', nlags='auto')\n",
        "        out['KPSS_stat'] = kpss_stat\n",
        "        out['KPSS_p'] = kpss_p\n",
        "        out['KPSS_stationary_at_5pct'] = kpss_p > 0.05\n",
        "    except Exception:\n",
        "        out['KPSS_stat'] = np.nan\n",
        "        out['KPSS_p'] = np.nan\n",
        "        out['KPSS_stationary_at_5pct'] = np.nan\n",
        "    return out\n",
        "\n",
        "stationarity_results = pd.DataFrame([\n",
        "    stationarity_test(agg['Weekly_Sales'], 'Aggregate Weekly_Sales (level)'),\n",
        "    stationarity_test(agg['log_sales'], 'Aggregate log(Weekly_Sales)'),\n",
        "    stationarity_test(agg['diff_sales'], 'Aggregate diff(Weekly_Sales)'),\n",
        "    stationarity_test(agg['diff_log_sales'], 'Aggregate diff(log(Weekly_Sales))')\n",
        "])\n",
        "\n",
        "for c in ['ADF_stat', 'ADF_p', 'KPSS_stat', 'KPSS_p']:\n",
        "    stationarity_results[c] = stationarity_results[c].map(lambda v: np.nan if pd.isna(v) else float(f'{v:.5f}'))\n",
        "\n",
        "stationarity_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-store summary: how often each variant looks stationary across stores\n",
        "\n",
        "def per_store_stationarity(col_name, use_log=False, use_diff=False):\n",
        "    rows = []\n",
        "    for store, g in df[['Store', 'Date', 'Weekly_Sales']].sort_values(['Store', 'Date']).groupby('Store'):\n",
        "        s = g['Weekly_Sales'].copy()\n",
        "        if use_log:\n",
        "            s = np.log(s)\n",
        "        if use_diff:\n",
        "            s = s.diff()\n",
        "        s = s.dropna()\n",
        "        if len(s) < 20:\n",
        "            continue\n",
        "        try:\n",
        "            adf_p = adfuller(s, autolag='AIC')[1]\n",
        "        except Exception:\n",
        "            adf_p = np.nan\n",
        "        try:\n",
        "            kpss_p = kpss(s, regression='c', nlags='auto')[1]\n",
        "        except Exception:\n",
        "            kpss_p = np.nan\n",
        "        rows.append({'Store': store, 'ADF_p': adf_p, 'KPSS_p': kpss_p})\n",
        "    res = pd.DataFrame(rows)\n",
        "    return {\n",
        "        'Series': col_name,\n",
        "        'Stores_tested': int(len(res)),\n",
        "        'ADF_stationary_%': float((res['ADF_p'] < 0.05).mean() * 100),\n",
        "        'KPSS_stationary_%': float((res['KPSS_p'] > 0.05).mean() * 100)\n",
        "    }\n",
        "\n",
        "store_stationarity_summary = pd.DataFrame([\n",
        "    per_store_stationarity('Store-level Weekly_Sales (level)', use_log=False, use_diff=False),\n",
        "    per_store_stationarity('Store-level log(Weekly_Sales)', use_log=True, use_diff=False),\n",
        "    per_store_stationarity('Store-level diff(Weekly_Sales)', use_log=False, use_diff=True),\n",
        "    per_store_stationarity('Store-level diff(log(Weekly_Sales))', use_log=True, use_diff=True),\n",
        "])\n",
        "\n",
        "for c in ['ADF_stationary_%', 'KPSS_stationary_%']:\n",
        "    store_stationarity_summary[c] = store_stationarity_summary[c].map(lambda x: float(f'{x:.2f}'))\n",
        "\n",
        "store_stationarity_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual rolling mean/std diagnostics for aggregate level/log series\n",
        "\n",
        "roll_window = 12\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
        "\n",
        "for i, (series, title) in enumerate([\n",
        "    (agg['Weekly_Sales'], 'Aggregate Weekly_Sales (level)'),\n",
        "    (agg['log_sales'], 'Aggregate log(Weekly_Sales)')\n",
        "]):\n",
        "    r = i\n",
        "    axes[r, 0].plot(agg['Date'], series, label='Series')\n",
        "    axes[r, 0].plot(agg['Date'], series.rolling(roll_window).mean(), label='Rolling mean (12)', linestyle='--')\n",
        "    axes[r, 0].set_title(f'{title} and Rolling Mean')\n",
        "    axes[r, 0].legend()\n",
        "\n",
        "    axes[r, 1].plot(agg['Date'], series.rolling(roll_window).std(), color='tab:orange')\n",
        "    axes[r, 1].set_title(f'{title} Rolling Std (12)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_DIR / 'stationarity_diagnostics.png', dpi=160)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering\n",
        "\n",
        "df_fe = df.copy()\n",
        "df_fe['year'] = df_fe['Date'].dt.year\n",
        "df_fe['month'] = df_fe['Date'].dt.month\n",
        "df_fe['weekofyear'] = df_fe['Date'].dt.isocalendar().week.astype(int)\n",
        "df_fe['quarter'] = df_fe['Date'].dt.quarter\n",
        "df_fe['is_month_start'] = df_fe['Date'].dt.is_month_start.astype(int)\n",
        "df_fe['is_month_end'] = df_fe['Date'].dt.is_month_end.astype(int)\n",
        "\n",
        "df_fe['week_sin'] = np.sin(2 * np.pi * df_fe['weekofyear'] / 52)\n",
        "df_fe['week_cos'] = np.cos(2 * np.pi * df_fe['weekofyear'] / 52)\n",
        "\n",
        "# Per-store autoregressive features\n",
        "for lag in [1, 2, 4, 8]:\n",
        "    df_fe[f'sales_lag_{lag}'] = df_fe.groupby('Store')['Weekly_Sales'].shift(lag)\n",
        "\n",
        "df_fe['sales_roll4_mean'] = (\n",
        "    df_fe.groupby('Store')['Weekly_Sales']\n",
        "    .shift(1)\n",
        "    .rolling(4)\n",
        "    .mean()\n",
        ")\n",
        "df_fe['sales_roll4_std'] = (\n",
        "    df_fe.groupby('Store')['Weekly_Sales']\n",
        "    .shift(1)\n",
        "    .rolling(4)\n",
        "    .std()\n",
        ")\n",
        "\n",
        "before_rows = len(df_fe)\n",
        "df_fe = df_fe.dropna().reset_index(drop=True)\n",
        "print('Rows dropped due to lag/rolling features:', before_rows - len(df_fe))\n",
        "print('Modeling shape:', df_fe.shape)\n",
        "\n",
        "df_fe.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-aware train/test split\n",
        "\n",
        "cutoff_date = df_fe['Date'].quantile(0.80)\n",
        "train_df = df_fe[df_fe['Date'] <= cutoff_date].copy()\n",
        "test_df = df_fe[df_fe['Date'] > cutoff_date].copy()\n",
        "\n",
        "print('Cutoff date:', cutoff_date)\n",
        "print('Train shape:', train_df.shape)\n",
        "print('Test shape:', test_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parametric Demand Equation\n",
        "We estimate a demand equation with store fixed effects and cluster-robust\n",
        "standard errors (clustered by `Store`) for more reliable coefficient inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normality-optimized demand equation with store fixed effects + robust inference\n",
        "\n",
        "eq_df = df_fe.copy()\n",
        "eq_df['trend'] = (eq_df['Date'] - eq_df['Date'].min()).dt.days\n",
        "train_eq = eq_df[eq_df['Date'] <= cutoff_date].copy()\n",
        "\n",
        "formula = (\n",
        "    'Weekly_Sales ~ Holiday_Flag + Temperature + Fuel_Price + CPI + Unemployment '\n",
        "    '+ trend + week_sin + week_cos + sales_lag_1 + sales_lag_4 + sales_roll4_mean '\n",
        "    '+ sales_roll4_std + C(Store)'\n",
        ")\n",
        "\n",
        "ols_model = smf.ols(formula=formula, data=train_eq).fit(\n",
        "    cov_type='cluster',\n",
        "    cov_kwds={'groups': train_eq['Store']}\n",
        ")\n",
        "\n",
        "coef_table = pd.DataFrame({\n",
        "    'coef': ols_model.params,\n",
        "    'std_err': ols_model.bse,\n",
        "    't_value': ols_model.tvalues,\n",
        "    'p_value': ols_model.pvalues\n",
        "})\n",
        "\n",
        "core_terms = [\n",
        "    'Intercept', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI',\n",
        "    'Unemployment', 'trend', 'week_sin', 'week_cos',\n",
        "    'sales_lag_1', 'sales_lag_4', 'sales_roll4_mean', 'sales_roll4_std'\n",
        "]\n",
        "coef_table = coef_table.loc[core_terms]\n",
        "coef_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parametric tests and equation rendering (on FE model residuals)\n",
        "resid = ols_model.resid\n",
        "exog = ols_model.model.exog\n",
        "\n",
        "bp_stat, bp_pvalue, bp_fstat, bp_fpvalue = het_breuschpagan(resid, exog)\n",
        "dw = durbin_watson(resid)\n",
        "jb_stat, jb_pvalue, skew, kurt = jarque_bera(resid)\n",
        "\n",
        "print('Breusch-Pagan LM statistic:', f'{bp_stat:.5f}')\n",
        "print('Breusch-Pagan LM p-value:', f'{bp_pvalue:.5f}')\n",
        "print('Breusch-Pagan F-statistic:', f'{bp_fstat:.5f}')\n",
        "print('Breusch-Pagan F p-value:', f'{bp_fpvalue:.5f}')\n",
        "print('Durbin-Watson statistic:', f'{dw:.5f}')\n",
        "print('Jarque-Bera statistic:', f'{jb_stat:.5f}')\n",
        "print('Jarque-Bera p-value:', f'{jb_pvalue:.5f}')\n",
        "print('Residual skewness:', f'{skew:.5f}')\n",
        "print('Residual kurtosis:', f'{kurt:.5f}')\n",
        "\n",
        "# Build demand equation text\n",
        "params = ols_model.params\n",
        "terms = []\n",
        "for term in [\n",
        "    'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "    'trend', 'week_sin', 'week_cos', 'sales_lag_1', 'sales_lag_4',\n",
        "    'sales_roll4_mean', 'sales_roll4_std'\n",
        "]:\n",
        "    terms.append(f\"({params[term]:.5f})*{term}\")\n",
        "\n",
        "equation = f\"Weekly_Sales = {params['Intercept']:.5f} + \" + ' + '.join(terms)\n",
        "print()\n",
        "print('Estimated demand equation (store FE absorbed, cluster-robust SEs):')\n",
        "print(equation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Learning Models\n",
        "We compare multiple non-linear regressors and an ensemble on a holdout period.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build model matrices\n",
        "\n",
        "target = 'Weekly_Sales'\n",
        "feature_cols = [\n",
        "    'Store', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "    'year', 'month', 'weekofyear', 'quarter', 'is_month_start', 'is_month_end',\n",
        "    'week_sin', 'week_cos', 'sales_lag_1', 'sales_lag_2', 'sales_lag_4', 'sales_lag_8',\n",
        "    'sales_roll4_mean', 'sales_roll4_std'\n",
        "]\n",
        "\n",
        "X_train = train_df[feature_cols].copy()\n",
        "y_train = train_df[target].copy()\n",
        "X_test = test_df[feature_cols].copy()\n",
        "y_test = test_df[target].copy()\n",
        "\n",
        "numeric_features = [c for c in feature_cols if c != 'Store']\n",
        "categorical_features = ['Store']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print('Train samples:', len(X_train), '| Test samples:', len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define candidate models\n",
        "models = {\n",
        "    'RandomForest': RandomForestRegressor(\n",
        "        n_estimators=800, max_depth=18, min_samples_leaf=2,\n",
        "        random_state=RANDOM_STATE, n_jobs=-1\n",
        "    ),\n",
        "    'ExtraTrees': ExtraTreesRegressor(\n",
        "        n_estimators=900,\n",
        "        max_depth=20,\n",
        "        min_samples_leaf=4,\n",
        "        min_samples_split=2,\n",
        "        max_features=1.0,\n",
        "        bootstrap=False,\n",
        "        random_state=RANDOM_STATE, n_jobs=-1\n",
        "    ),\n",
        "    'HistGradientBoosting': HistGradientBoostingRegressor(\n",
        "        learning_rate=0.05, max_depth=8, max_iter=500,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "}\n",
        "\n",
        "if HAS_XGB:\n",
        "    models['XGBoost'] = XGBRegressor(\n",
        "        n_estimators=600,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.04,\n",
        "        subsample=0.85,\n",
        "        colsample_bytree=0.85,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=1.0,\n",
        "        random_state=RANDOM_STATE,\n",
        "        objective='reg:squarederror',\n",
        "        n_jobs=4\n",
        "    )\n",
        "\n",
        "# Voting regressor with strongest tree families\n",
        "voters = [\n",
        "    ('rf', clone(models['RandomForest'])),\n",
        "    ('et', clone(models['ExtraTrees']))\n",
        "]\n",
        "if HAS_XGB:\n",
        "    voters.append(('xgb', clone(models['XGBoost'])))\n",
        "\n",
        "models['VotingRegressor'] = VotingRegressor(voters)\n",
        "\n",
        "list(models.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate models\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-6))) * 100\n",
        "\n",
        "results = []\n",
        "predictions = {}\n",
        "trained_pipelines = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    pipe = Pipeline([\n",
        "        ('prep', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    pred = pipe.predict(X_test)\n",
        "\n",
        "    trained_pipelines[name] = pipe\n",
        "    predictions[name] = pred\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'MAE': mean_absolute_error(y_test, pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, pred)),\n",
        "        'MAPE_%': mape(y_test, pred),\n",
        "        'Accuracy_%': 100 - mape(y_test, pred),\n",
        "        'R2': r2_score(y_test, pred)\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values('RMSE').reset_index(drop=True)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot model comparison\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.barplot(data=results_df, x='RMSE', y='Model', palette='magma', ax=ax[0])\n",
        "ax[0].set_title('RMSE by Model (Lower is better)')\n",
        "\n",
        "sns.barplot(data=results_df, x='R2', y='Model', palette='viridis', ax=ax[1])\n",
        "ax[1].set_title('R2 by Model (Higher is better)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_DIR / 'model_comparison.png', dpi=160)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best model diagnostics\n",
        "best_model_name = results_df.loc[0, 'Model']\n",
        "best_pred = predictions[best_model_name]\n",
        "residuals = y_test.values - best_pred\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].scatter(y_test, best_pred, alpha=0.5)\n",
        "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "axes[0].set_title(f'Actual vs Predicted ({best_model_name})')\n",
        "axes[0].set_xlabel('Actual')\n",
        "axes[0].set_ylabel('Predicted')\n",
        "\n",
        "sns.histplot(residuals, bins=40, kde=True, ax=axes[1], color='#ff7f0e')\n",
        "axes[1].set_title('Residual Distribution')\n",
        "\n",
        "axes[2].plot(test_df['Date'].values, residuals, color='#2ca02c')\n",
        "axes[2].axhline(0, color='black', linewidth=1)\n",
        "axes[2].set_title('Residuals Over Time')\n",
        "axes[2].set_xlabel('Date')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_DIR / 'best_model_diagnostics.png', dpi=160)\n",
        "plt.show()\n",
        "\n",
        "print('Best model:', best_model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (tree models where available)\n",
        "importance_rows = []\n",
        "\n",
        "for name, pipe in trained_pipelines.items():\n",
        "    model = pipe.named_steps['model']\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        feat_names = pipe.named_steps['prep'].get_feature_names_out()\n",
        "        importances = model.feature_importances_\n",
        "        top_idx = np.argsort(importances)[-12:][::-1]\n",
        "        for idx in top_idx:\n",
        "            importance_rows.append({\n",
        "                'Model': name,\n",
        "                'Feature': feat_names[idx],\n",
        "                'Importance': importances[idx]\n",
        "            })\n",
        "\n",
        "imp_df = pd.DataFrame(importance_rows)\n",
        "if len(imp_df):\n",
        "    top_imp = imp_df.sort_values('Importance', ascending=False).head(20)\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    sns.barplot(data=top_imp, x='Importance', y='Feature', hue='Model')\n",
        "    plt.title('Top Feature Importances Across Tree-Based Models')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_DIR / 'feature_importance.png', dpi=160)\n",
        "    plt.show()\n",
        "else:\n",
        "    print('No feature importances available for current model set.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af110392",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Notebook run complete. Generated artifacts in:', OUT_DIR)\n",
        "print('Files:')\n",
        "for p in sorted(OUT_DIR.glob('*')):\n",
        "    print('-', p.name)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
